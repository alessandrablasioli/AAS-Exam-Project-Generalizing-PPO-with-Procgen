{"policy_loss": 0.07378668338060379, "value_loss": 0.0026736839208751917, "entropy": 2.687516212463379, "_timestamp": 1757212475.6555414, "_runtime": 42633.070177316666, "_step": 2972, "update": 999, "eval_reward_mean": 0.3, "eval_reward_std": 0.9000000000000001, "episode_reward": 0.0, "episode_length": 24, "episode_reward_mean": 0.0, "episode_length_mean": 44.5, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "num_episodes": 2, "_wandb": {"runtime": 42632}}