{"policy_loss": -0.028881147503852844, "value_loss": 0.0406700074672699, "entropy": 2.6510677337646484, "_timestamp": 1757094005.1960971, "_runtime": 4477.247150182724, "_step": 1028, "update": 339, "eval_reward_mean": 1.0, "eval_reward_std": 3.0, "episode_reward": 0.0, "episode_length": 33, "episode_reward_mean": 0.0, "episode_length_mean": 123.0, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "num_episodes": 2, "_wandb": {"runtime": 4657}}