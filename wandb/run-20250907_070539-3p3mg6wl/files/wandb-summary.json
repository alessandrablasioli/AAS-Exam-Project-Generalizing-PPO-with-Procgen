{"policy_loss": -0.05905484780669212, "value_loss": 0.006180546712130308, "entropy": 2.690950393676758, "_timestamp": 1757229433.7656686, "_runtime": 7894.51698756218, "_step": 2984, "update": 999, "eval_reward_mean": 0.0, "eval_reward_std": 0.0, "episode_reward": 0.0, "episode_length": 250, "episode_reward_mean": 0.0, "episode_length_mean": 250.0, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "num_episodes": 1, "_wandb": {"runtime": 7893}}