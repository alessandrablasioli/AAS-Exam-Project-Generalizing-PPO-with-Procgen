{"policy_loss": 0.03851727768778801, "value_loss": 0.006187130697071552, "entropy": 2.664976119995117, "_timestamp": 1757103015.5903144, "_runtime": 5021.5077793598175, "_step": 1860, "update": 610, "eval_reward_mean": 2.0, "eval_reward_std": 4.0, "episode_reward": 0.0, "episode_length": 32, "episode_reward_mean": 0.0, "episode_length_mean": 32.0, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "num_episodes": 1, "_wandb": {"runtime": 5030}}