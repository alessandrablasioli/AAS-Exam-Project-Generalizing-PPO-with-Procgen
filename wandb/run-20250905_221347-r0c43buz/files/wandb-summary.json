{"policy_loss": -0.32818153500556946, "value_loss": 0.7105033993721008, "entropy": 2.644507646560669, "_timestamp": 1757112674.4334073, "_runtime": 9446.440575361252, "_step": 3073, "update": 999, "eval_reward_mean": 0.0, "eval_reward_std": 0.0, "episode_reward": 10.0, "episode_length": 239, "episode_reward_mean": 10.0, "episode_length_mean": 239.0, "episode_reward_max": 10.0, "episode_reward_min": 10.0, "num_episodes": 1, "_wandb": {"runtime": 9445}}