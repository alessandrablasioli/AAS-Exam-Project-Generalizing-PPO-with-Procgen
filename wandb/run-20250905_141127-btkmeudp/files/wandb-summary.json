{"policy_loss": -0.32074880599975586, "value_loss": 0.001691102865152061, "entropy": 2.621762990951538, "_timestamp": 1757079479.6835246, "_runtime": 5192.413472652435, "_step": 1148, "update": 392, "eval_reward_mean": 0.0, "eval_reward_std": 0.0, "episode_reward": 0.0, "episode_length": 53, "episode_reward_mean": 0.0, "episode_length_mean": 53.0, "episode_reward_max": 0.0, "episode_reward_min": 0.0, "num_episodes": 1, "_wandb": {"runtime": 5192}}